# -*- coding: utf-8 -*-
"""Mistral-7b-DZ_Startups.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2CG60eC8SFm9veFQpI_0wEXal9VNFR0
"""

import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
import transformers
model_name = "bn22/Mistral-7B-Instruct-v0.1-sharded"
device = "cuda"

bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    torch_dtype=torch.bfloat16,
    quantization_config=bnb_config,
    device_map='auto'
)

config = PeftConfig.from_pretrained("ayoubkirouane/Mistral-7b-DZ_Startups")
model = PeftModel.from_pretrained(model, "ayoubkirouane/Mistral-7b-DZ_Startups")

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.bos_token_id = 1
stop_token_ids = [0]

def chatbot(text) :
  text = f"[INST] {text} [/INST]"
  encoded = tokenizer(text, return_tensors="pt", add_special_tokens=False)
  model_input = encoded
  model.to(device)
  generated_ids = model.generate(**model_input, max_new_tokens=512, do_sample=True)
  decoded = tokenizer.batch_decode(generated_ids)
  return decoded[0].replace(text , "")

import gradio as gr
# Create a Gradio interface
input_text = gr.Textbox(textarea=True, label="User Input")
output_text = gr.Textbox(textarea=True, label="Chatbot Response")

iface = gr.Interface(fn=chatbot,
                     inputs=input_text,
                     outputs=output_text ,
                     allow_flagging=False ,
                     examples=["How to start a startup in Algeria?"])

# Run the Gradio app
iface.launch(share=True , debug=True)
